{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-23T11:02:09.937205Z",
     "start_time": "2024-07-23T11:02:05.635272Z"
    }
   },
   "source": [
    "import logging\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from lightning.pytorch.utilities.types import TRAIN_DATALOADERS\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "from monai.inferers import sliding_window_inference\n",
    "from skimage import measure\n",
    "\n",
    "import experiments_items.nets\n",
    "from monai.metrics.meandice import compute_dice\n",
    "from config.config import Config\n",
    "from preprocessing.covid_dataset import CovidDataset\n",
    "import monai.data\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import DiceLoss\n",
    "from preprocessing.transforms import get_hrct_transforms, get_cbct_transforms, \\\n",
    "    get_val_hrct_transforms, get_val_cbct_transforms\n",
    "from utils.custom_callbacks import CustomTimingCallback\n",
    "from utils.helpers import load_images_from_path, check_dataset\n",
    "from config.constants import (ZENODO_COVID_CASES_PATH, ZENODO_INFECTION_MASKS_PATH, SEED, VALIDATION_INFERENCE_ROI_SIZE,\n",
    "                              SPATIAL_SIZE,\n",
    "                              ZENODO_LUNG_MASKS_PATH, EXPERIMENTS_PATH, COVID_PREPROCESSED_CASES_PATH,\n",
    "                              INFECTION_PREPROCESSED_MASKS_PATH, SPACING)\n",
    "import torch\n",
    "import numpy as np\n",
    "from monai.metrics import DiceMetric\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import matplotlib.pyplot as plt\n",
    "import pyvista as pv\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy.ndimage as ndi\n",
    "import plotly.graph_objects as go\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 13:02:08.408334: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-23 13:02:08.983374: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T11:02:09.952189Z",
     "start_time": "2024-07-23T11:02:09.938330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_covid_volumes(ground_truth, predictions, save_path=\"test.png\"):\n",
    "    \"\"\"\n",
    "    Plots contours of ground truth and fills areas of predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - ground_truth: A 3D numpy array (or metatensor) with binary values (0s and 1s) representing the ground truth.\n",
    "    - predictions: A 3D numpy array (or metatensor) with binary values (0s and 1s) representing the model's predictions.\n",
    "    - save_path: A string path to save the resulting plotly figure.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert tensors to numpy arrays if they are not already\n",
    "    if torch.is_tensor(ground_truth):\n",
    "        ground_truth = ground_truth.cpu().numpy()\n",
    "    if torch.is_tensor(predictions):\n",
    "        predictions = predictions.cpu().numpy()\n",
    "\n",
    "    # Ensure input arrays are binary (0s and 1s)\n",
    "    assert np.all(np.isin(ground_truth, [0, 1])), \"Ground truth should be binary (0s and 1s)\"\n",
    "    assert np.all(np.isin(predictions, [0, 1])), \"Predictions should be binary (0s and 1s)\"\n",
    "\n",
    "    # Create the figure\n",
    "    fig = go.Figure()\n",
    "        \n",
    "    # paint the prediction volume, such that we paint the whole volume with all slices, where the values are 1\n",
    "    fig.update(data=[\n",
    "                go.Volume(\n",
    "                    x=np.repeat(np.arange(predictions.shape[0]), predictions.shape[1] * predictions.shape[2]),\n",
    "                    y=np.tile(np.repeat(np.arange(predictions.shape[1]), predictions.shape[2]), predictions.shape[0]),\n",
    "                    z=np.tile(np.arange(predictions.shape[2]), predictions.shape[0] * predictions.shape[1]),\n",
    "                    value=predictions.flatten(),\n",
    "                    opacity=0.1, # adjust for visualization\n",
    "                    surface_count=17, # adjust for visualization\n",
    "                    colorscale='Viridis' # or any other color scale\n",
    "                )\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='3D Metatensor Visualization',\n",
    "        scene=dict(\n",
    "            xaxis_title='X Axis',\n",
    "            yaxis_title='Y Axis',\n",
    "            zaxis_title='Z Axis'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    plt.savefig(save_path)   \n",
    "    \n",
    "\n",
    "def plot_3d_volumes(ground_truth, prediction, save_path):\n",
    "    \"\"\"\n",
    "    Plot 3D volumes of the ground truth and prediction tensors and save the plot.\n",
    "    \n",
    "    Parameters:\n",
    "    ground_truth (torch.Tensor): 4D tensor (CHWD) representing the ground truth.\n",
    "    prediction (torch.Tensor): 4D tensor (CHWD) representing the prediction.\n",
    "    save_path (str): Path to save the 3D visualization.\n",
    "    \"\"\"\n",
    "    # Extract the volume data from the tensors and convert to numpy arrays\n",
    "    gt_volume = ground_truth[0].cpu().numpy()\n",
    "    pred_volume = prediction[0].cpu().numpy()\n",
    "    \n",
    "    # Find the indices of the 'ones' in the ground truth and prediction volumes\n",
    "    gt_indices = np.argwhere(gt_volume == 1)\n",
    "    pred_indices = np.argwhere(pred_volume == 1)\n",
    "    \n",
    "    # Set up the figure and 3D axis\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot the ground truth volume\n",
    "    if gt_indices.size > 0:\n",
    "        gt_x, gt_y, gt_z = gt_indices[:, 0], gt_indices[:, 1], gt_indices[:, 2]\n",
    "        ax.scatter(gt_x, gt_y, gt_z, color='blue', alpha=0.01, label='Ground Truth')\n",
    "\n",
    "    # Plot the prediction volume\n",
    "    if pred_indices.size > 0:\n",
    "        pred_x, pred_y, pred_z = pred_indices[:, 0], pred_indices[:, 1], pred_indices[:, 2]\n",
    "        ax.scatter(pred_x, pred_y, pred_z, color='red', alpha=0.2, label='Prediction')\n",
    "\n",
    "    # Set labels\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_title('3D Volumes of Ground Truth and Prediction')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Save the plot to the specified path\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "def plot_3d_volumes2(ground_truth, prediction, save_path):\n",
    "    \"\"\"\n",
    "    Plot 3D volumes of the ground truth and prediction tensors and save the plot with volume rendering.\n",
    "    Ground truth is visualized as contours, and prediction is visualized as points.\n",
    "    \n",
    "    Parameters:\n",
    "    ground_truth (torch.Tensor): 4D tensor (CHWD) representing the ground truth.\n",
    "    prediction (torch.Tensor): 4D tensor (CHWD) representing the prediction.\n",
    "    save_path (str): Path to save the 3D visualization.\n",
    "    \"\"\"\n",
    "    # Extract the volume data from the tensors and convert to numpy arrays\n",
    "    gt_volume = ground_truth[0].cpu().numpy()\n",
    "    pred_volume = prediction[0].cpu().numpy()\n",
    "\n",
    "    # Downsample the volumes to speed up processing\n",
    "    def downsample_volume(volume, factor):\n",
    "        zoom_factors = [1/factor, 1/factor, 1]  # Downsample spatial dimensions\n",
    "        return ndi.zoom(volume, zoom_factors, order=1)  # Bilinear interpolation\n",
    "\n",
    "    gt_volume_ds = downsample_volume(gt_volume, 4)\n",
    "    pred_volume_ds = downsample_volume(pred_volume, 4)\n",
    "\n",
    "    # Set up the figure and 3D axis\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "  # Function to add contours to the plot\n",
    "    def add_volume(ax, volume, color, alpha=0.3):\n",
    "        # Compute contours\n",
    "        verts, faces, _, _ = measure.marching_cubes(volume, level=0.5)\n",
    "        mesh = Poly3DCollection(verts[faces], alpha=alpha, facecolors=color, linewidths=0.1)\n",
    "        ax.add_collection3d(mesh)\n",
    "\n",
    "    # Plot the ground truth contours\n",
    "    if np.any(gt_volume_ds):\n",
    "        add_volume(ax, gt_volume_ds, 'blue', alpha=0.3)\n",
    "    \n",
    "    # Plot the prediction points\n",
    "    gt_indices = np.argwhere(gt_volume_ds == 1)\n",
    "    if gt_indices.size > 0:\n",
    "        gt_x, gt_y, gt_z = gt_indices[:, 0], gt_indices[:, 1], gt_indices[:, 2]\n",
    "        ax.scatter(gt_x, gt_y, gt_z, color='blue', alpha=0.01, label='Ground Truth')\n",
    "    \n",
    "    # Set labels and aspect\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_title('3D Volumes of Ground Truth and Prediction')\n",
    "\n",
    "    # Set aspect ratio\n",
    "    ax.set_box_aspect([gt_volume_ds.shape[0], gt_volume_ds.shape[1], gt_volume_ds.shape[2]])\n",
    "\n",
    "    # Add a legend\n",
    "    ax.legend()\n",
    "\n",
    "    # Save the plot to the specified path\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "def plot_3d_volumes3(ground_truth, prediction, save_path):\n",
    "    # using plotply\n",
    "    # Extract the volume data from the tensors and convert to numpy arrays\n",
    "    gt_volume = ground_truth[0].cpu().numpy()\n",
    "    pred_volume = prediction[0].cpu().numpy()\n",
    "    \n",
    "    # Find the indices of the 'ones' in the ground truth and prediction volumes\n",
    "    gt_indices = np.argwhere(gt_volume == 1)\n",
    "    pred_indices = np.argwhere(pred_volume == 1)\n",
    "    \n",
    "    # Set up the figure and 3D axis\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Plot the prediction volume\n",
    "    if pred_indices.size > 0:\n",
    "        pred_x, pred_y, pred_z = pred_indices[:, 0], pred_indices[:, 1], pred_indices[:, 2]\n",
    "        fig.add_trace(go.Scatter3d(x=pred_x, y=pred_y, z=pred_z, mode='markers', opacity=0.1, marker=dict(size=2, color='red'), name='Prediction'))\n",
    "\n",
    "    if gt_indices.size > 0:\n",
    "        print(f\"shape of gt_indices: {gt_indices.shape}\")\n",
    "        pred_x, pred_y, pred_z = gt_indices[:, 0], gt_indices[:, 1], gt_indices[:, 2]\n",
    "        fig.add_trace(go.Scatter3d(x=pred_x, y=pred_y, z=pred_z, mode='markers', opacity=0.1, marker=dict(size=2, color='blue'), name='Ground Truth'))\n",
    "\n",
    "    # Set layout\n",
    "    fig.update_layout(scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z'), title='3D Volumes of Ground Truth and Prediction')\n",
    "        \n",
    "    # plot\n",
    "    fig.show()\n",
    "    \n",
    "def burn_masks_in_ct(ct,mask,predictions, path_to_save=None):\n",
    "    # Definir los valores de las clases\n",
    "    BACKGROUND = 0\n",
    "    CLASS1 = 1\n",
    " \n",
    "    # Cambiar los valores de la matriz \"predictions_without_black\" a los valores de las clases\n",
    "    predictions_without_black = np.where(predictions == BACKGROUND, np.nan, CLASS1)\n",
    " \n",
    "    # Crear el mapa de colores personalizado\n",
    "    cmap = ListedColormap(['yellow', 'black'])\n",
    " \n",
    "    # Crear la figura y los ejes\n",
    "    dpi = 80\n",
    "    figsize = 512 / dpi\n",
    " \n",
    "    # Crear la figura y los ejes\n",
    "    fig, ax = plt.subplots(figsize=(figsize + 1, figsize + 1.2), dpi=dpi)\n",
    " \n",
    "    # Mostrar la imagen con el mapa de colores personalizado\n",
    "    ax.imshow(ct, cmap=\"gray\")\n",
    "    ax.imshow(predictions_without_black, alpha=0.6, cmap=cmap)\n",
    " \n",
    "    # Encontrar los contornos de la máscara\n",
    "    contours = measure.find_contours(mask)\n",
    " \n",
    "    # Dibujar los contornos\n",
    "    for contour in contours:\n",
    "        ax.plot(contour[:, 1], contour[:, 0], linewidth=1, color=\"blue\")\n",
    " \n",
    "    # Añadir el título\n",
    "    #ax.set_title(\"Dice score: {:.3%}\".format(dice_score), fontsize=16)\n",
    " \n",
    "    # Crear la leyenda con texto y colores\n",
    "    class_patches = [Patch(color='yellow', label='Prediction')]\n",
    "    line_patches = [Patch(color='blue', label='Mask contour')]\n",
    " \n",
    "    legend = ax.legend(handles=class_patches + line_patches, loc='upper left')\n",
    "    legend.get_frame().set_facecolor('white')\n",
    "    legend.get_frame().set_linewidth(0.0)\n",
    "    legend.get_frame().set_alpha(0.8)  # Agregar transparencia al marco\n",
    "    legend.get_texts()[0].set_fontsize(8)  # Ajustar el tamaño del texto\n",
    "    legend.get_texts()[1].set_fontsize(8)\n",
    " \n",
    "    # Quitar los ejes xticks e yticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    " \n",
    "    # Guardar la figura\n",
    "    plt.savefig(path_to_save, bbox_inches='tight')\n",
    " \n",
    "    # Cerrar la figura\n",
    "    plt.close()\n"
   ],
   "id": "de14f00d5f894896",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T11:23:22.520693Z",
     "start_time": "2024-07-23T11:23:22.498902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Net(L.pytorch.LightningModule):\n",
    "    def __init__(self, learning_rate: float, model: torch.nn.Module, loss_function: torch.nn, volumes_path: str,\n",
    "                 masks_path: str, experiment_name: str):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # volumes paths\n",
    "        self.volumes_path = volumes_path\n",
    "        self.masks_path = masks_path\n",
    "\n",
    "        # Model, loss function and learning rate\n",
    "        self.model = model\n",
    "        print(f\"Using model: {type(self.model)}\")\n",
    "        self.loss_function = loss_function\n",
    "        print(f\"Using loss: {type(self.loss_function)}\")\n",
    "        self.learning_rate = learning_rate\n",
    "        print(f\"Using lr: {learning_rate}\")\n",
    "\n",
    "        self.experiment_name = experiment_name\n",
    "\n",
    "        self.save_hyperparameters(ignore=[\"model\", \"loss_function\", \"volumes_path\", \"masks_path\", \"experiment_name\"])\n",
    "\n",
    "        # Define the post-processing transforms\n",
    "        self.post_pred = monai.transforms.Compose(\n",
    "            [monai.transforms.EnsureType(data_type='tensor'), monai.transforms.Activations(sigmoid=True), monai.transforms.AsDiscrete(threshold=0.5)])\n",
    "        self.post_label = monai.transforms.Compose([monai.transforms.AsDiscrete(threshold=0.5)])\n",
    "\n",
    "        # Dice metric\n",
    "        self.dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "        self.train_dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "\n",
    "        # Surface dice metric\n",
    "        self.surface_dice_metric = monai.metrics.SurfaceDiceMetric(include_background=True, distance_metric=\"euclidean\", class_thresholds=[1.0])\n",
    "        self.train_surface_dice_metric = monai.metrics.SurfaceDiceMetric(include_background=True, distance_metric=\"euclidean\", class_thresholds=[1.0])\n",
    "\n",
    "        # Haussdorf metric\n",
    "        self.haussdorf_metric = monai.metrics.HausdorffDistanceMetric(include_background=True, distance_metric=\"euclidean\", percentile=95)\n",
    "        self.train_haussdorf_metric = monai.metrics.HausdorffDistanceMetric(include_background=True, distance_metric=\"euclidean\", percentile=95)\n",
    "\n",
    "        # IoU metric\n",
    "        self.iou_metric = monai.metrics.MeanIoU(include_background=True)\n",
    "        self.train_iou_metric = monai.metrics.MeanIoU(include_background=True)\n",
    "\n",
    "        # Best validation dice and epoch\n",
    "        self.best_val_dice = 0\n",
    "        self.best_val_epoch = 0\n",
    "\n",
    "        # Losses lists\n",
    "        self.validation_step_outputs = []\n",
    "        self.train_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "        # CSV data frame for metrics\n",
    "        self.train_val_dump_data_frame = []\n",
    "        self.test_dump_data_frame = []\n",
    "\n",
    "        # Paths lists for datasets\n",
    "        self.test_paths = None\n",
    "        self.val_paths = None\n",
    "        self.train_paths = None\n",
    "\n",
    "        # Datasets\n",
    "        self.training_ds = None\n",
    "        self.validation_ds = None\n",
    "        self.test_ds = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # Load images and masks\n",
    "        logging.info(f\"Loading images from {self.volumes_path} and masks from {self.masks_path}\")\n",
    "        images = load_images_from_path(self.volumes_path)\n",
    "        labels = load_images_from_path(self.masks_path)\n",
    "\n",
    "        # Take only the images that are from Mosmed\n",
    "        train_images = [image for image in images if \"radiopaedia\" not in image and \"coronacases\" not in image]\n",
    "        train_labels = [label for label in labels if \"radiopaedia\" not in label and \"coronacases\" not in label]\n",
    "\n",
    "        # Convert images and masks to a list of dictionaries with keys \"img\" and \"mask\"\n",
    "        data_train_dicts = np.array([{\"img\": img, \"mask\": mask} for img, mask in zip(train_images, train_labels)])\n",
    "        logging.debug(data_train_dicts)\n",
    "\n",
    "        # Shuffle the data\n",
    "        shuffler = np.random.RandomState(SEED)\n",
    "        shuffler.shuffle(data_train_dicts)\n",
    "        data_train_dicts = list(data_train_dicts)\n",
    "\n",
    "        # Split the training data into training and validation\n",
    "        val_split = int(len(data_train_dicts) * 0.2)\n",
    "\n",
    "        self.train_paths = data_train_dicts[val_split:]\n",
    "        self.val_paths = data_train_dicts[:val_split]\n",
    "\n",
    "        # Take coronacases and radiopeadia images for testing\n",
    "        test_images = [image for image in images if \"radiopaedia\" in image or \"coronacases\" in image]\n",
    "        test_labels = [label for label in labels if \"radiopaedia\" in label or \"coronacases\" in label]\n",
    "        self.test_paths = np.array([{\"img\": img, \"mask\": mask} for img, mask in zip(test_images, test_labels)])\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            # Define the CovidDataset instances for training, validation, and test\n",
    "            self.training_ds = CovidDataset(volumes=self.train_paths, hrct_transform=get_hrct_transforms(),\n",
    "                                            cbct_transform=get_cbct_transforms())\n",
    "            self.validation_ds = CovidDataset(volumes=self.val_paths, hrct_transform=get_val_hrct_transforms(),\n",
    "                                              cbct_transform=get_val_cbct_transforms())\n",
    "            # Check the dataset\n",
    "            print(\"Checking the validation dataset\")\n",
    "            # check_dataset(self.validation_ds)\n",
    "\n",
    "        if stage == \"validate\" or stage is None:\n",
    "            self.validation_ds = CovidDataset(volumes=self.val_paths, hrct_transform=get_val_hrct_transforms(),\n",
    "                                              cbct_transform=get_val_cbct_transforms())\n",
    "            # Check the dataset\n",
    "            print(\"Checking the validation dataset\")\n",
    "            # check_dataset(self.validation_ds)\n",
    "\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_ds = CovidDataset(volumes=self.test_paths, hrct_transform=get_val_hrct_transforms(),\n",
    "                                        cbct_transform=get_val_cbct_transforms())\n",
    "            # Check the dataset\n",
    "            print(\"Checking the test dataset\")\n",
    "            # check_dataset(self.test_ds)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataloader = DataLoader(self.training_ds, batch_size=1, shuffle=True, num_workers=4)\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataloader = DataLoader(self.validation_ds, batch_size=1, num_workers=4)\n",
    "        return val_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_dataloader = DataLoader(self.test_ds, batch_size=1, num_workers=4)\n",
    "        return test_dataloader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch[\"img\"], batch[\"mask\"]\n",
    "\n",
    "        # Forward pass\n",
    "        raw_outputs = self.forward(inputs)\n",
    "        loss = self.loss_function(raw_outputs, labels)\n",
    "        self.log(\"ts_loss\", loss, on_step=True, on_epoch=False, prog_bar=True, logger=False)\n",
    "        outputs = [self.post_pred(i) for i in decollate_batch(raw_outputs)]\n",
    "        labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
    "\n",
    "        # Dice metric\n",
    "        self.train_dice_metric(y_pred=outputs, y=labels)\n",
    "        self.log(\"ts_dice\", self.train_dice_metric.aggregate().item(), on_step=True, on_epoch=False, prog_bar=True, logger=False)\n",
    "\n",
    "        # NSDice Metric\n",
    "        self.train_surface_dice_metric(y_pred=outputs, y=labels, spacing=SPACING)\n",
    "        self.log(\"ts_surface_dice\", self.train_surface_dice_metric.aggregate().item(), on_step=True, on_epoch=False, prog_bar=True, logger=False)\n",
    "\n",
    "        # Haussdorf Metric\n",
    "        self.train_haussdorf_metric(y_pred=outputs, y=labels, spacing=SPACING)\n",
    "        self.log(\"ts_hd95\", self.train_haussdorf_metric.aggregate().item(), on_step=True, on_epoch=False, prog_bar=True, logger=False)\n",
    "\n",
    "        # IoU Metric\n",
    "        self.train_iou_metric(y_pred=outputs, y=labels)\n",
    "        self.log(\"ts_iou\", self.train_iou_metric.aggregate().item(), on_step=True, on_epoch=False, prog_bar=True, logger=False)\n",
    "\n",
    "        # Store the loss\n",
    "        train_loss_dictionary = {\"loss\": loss}\n",
    "        self.train_step_outputs.append(train_loss_dictionary)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        # Loss\n",
    "        avg_loss = torch.stack([i[\"loss\"] for i in self.train_step_outputs]).mean()\n",
    "        self.train_step_outputs.clear()\n",
    "\n",
    "        # Dice\n",
    "        mean_train_dice = self.train_dice_metric.aggregate().item()\n",
    "        self.train_dice_metric.reset()\n",
    "\n",
    "        # NSDice\n",
    "        mean_train_surface_dice = self.train_surface_dice_metric.aggregate().item()\n",
    "        self.train_surface_dice_metric.reset()\n",
    "\n",
    "        # Haussdorf\n",
    "        mean_train_haussdorf = self.train_haussdorf_metric.aggregate().item()\n",
    "        self.train_haussdorf_metric.reset()\n",
    "\n",
    "        # IoU\n",
    "        mean_train_iou = self.train_iou_metric.aggregate().item()\n",
    "        self.train_iou_metric.reset()\n",
    "\n",
    "        # Log the metrics\n",
    "        self.log_dict({\n",
    "            \"train_loss\": avg_loss,\n",
    "            \"train_dice\": mean_train_dice,\n",
    "            \"train_surface_dice\": mean_train_surface_dice,\n",
    "            \"train_haussdorf\": mean_train_haussdorf,\n",
    "            \"train_iou\": mean_train_iou\n",
    "        }, on_epoch=True, on_step=False, prog_bar=True)\n",
    "\n",
    "        # Save the metrics to a pandas dataframe\n",
    "        self.train_val_dump_data_frame[-1].update({\n",
    "            \"train_loss\": avg_loss.item(),\n",
    "            \"train_dice\": mean_train_dice,\n",
    "            \"train_surface_dice\": mean_train_surface_dice,\n",
    "            \"train_haussdorf\": mean_train_haussdorf,\n",
    "            \"train_iou\": mean_train_iou\n",
    "        })\n",
    "\n",
    "        # Log the metrics to tensorboard\n",
    "        self.logger.experiment.add_scalars(\"losses\", {\"train\": avg_loss}, self.current_epoch)\n",
    "        self.logger.experiment.add_scalars(\"dice\", {\"train\": mean_train_dice}, self.current_epoch)\n",
    "        self.logger.experiment.add_scalars(\"surface_dice\", {\"train\": mean_train_surface_dice}, self.current_epoch)\n",
    "        self.logger.experiment.add_scalars(\"haussdorf\", {\"train\": mean_train_haussdorf}, self.current_epoch)\n",
    "        self.logger.experiment.add_scalars(\"iou\", {\"train\": mean_train_iou}, self.current_epoch)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch[\"img\"], batch[\"mask\"]\n",
    "\n",
    "        # Inference\n",
    "        roi_size = VALIDATION_INFERENCE_ROI_SIZE\n",
    "        sw_batch_size = 4\n",
    "        outputs = sliding_window_inference(inputs, roi_size, sw_batch_size, self.forward, overlap=0.6)\n",
    "        loss = self.loss_function(outputs, labels)\n",
    "        outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
    "        labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
    "        self.validation_step_outputs.append({\"val_loss\": loss})\n",
    "\n",
    "        # Validation metrics\n",
    "        self.dice_metric(y_pred=outputs, y=labels)\n",
    "        self.surface_dice_metric(y_pred=outputs, y=labels, spacing=SPACING)\n",
    "        self.haussdorf_metric(y_pred=outputs, y=labels, spacing=SPACING)\n",
    "        self.iou_metric(y_pred=outputs, y=labels)\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in self.validation_step_outputs]).mean()\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "        # Dice\n",
    "        mean_val_dice = self.dice_metric.aggregate().item()\n",
    "        self.dice_metric.reset()\n",
    "\n",
    "        # NSDice\n",
    "        mean_val_surface_dice = self.surface_dice_metric.aggregate().item()\n",
    "        self.surface_dice_metric.reset()\n",
    "\n",
    "        # Haussdorf\n",
    "        mean_val_haussdorf = self.haussdorf_metric.aggregate().item()\n",
    "        self.haussdorf_metric.reset()\n",
    "\n",
    "        # IoU\n",
    "        mean_val_iou = self.iou_metric.aggregate().item()\n",
    "        self.iou_metric.reset()\n",
    "\n",
    "        # Log the metrics\n",
    "        self.log_dict({\n",
    "            \"val_loss\": avg_loss.item(),\n",
    "            \"val_dice\": mean_val_dice,\n",
    "            \"val_surface_dice\": mean_val_surface_dice,\n",
    "            \"val_haussdorf\": mean_val_haussdorf,\n",
    "            \"val_iou\": mean_val_iou\n",
    "        }, prog_bar=True, on_epoch=True, on_step=False)\n",
    "\n",
    "        # Save the metrics to a pandas dataframe\n",
    "        self.train_val_dump_data_frame.append({\n",
    "            \"epoch\": self.current_epoch,\n",
    "            \"val_loss\": avg_loss.item(),\n",
    "            \"val_dice\": mean_val_dice,\n",
    "            \"val_surface_dice\": mean_val_surface_dice,\n",
    "            \"val_haussdorf\": mean_val_haussdorf,\n",
    "            \"val_iou\": mean_val_iou\n",
    "        })\n",
    "\n",
    "        # Log the metrics to tensorboard\n",
    "        self.logger.experiment.add_scalars(\"losses\", {\"val_loss\": avg_loss}, self.current_epoch)\n",
    "        self.logger.experiment.add_scalars(\"dice\", {\"val_dice\": mean_val_dice}, self.current_epoch)\n",
    "        self.logger.experiment.add_scalars(\"surface_dice\", {\"val_surface_dice\": mean_val_surface_dice}, self.current_epoch)\n",
    "        self.logger.experiment.add_scalars(\"haussdorf\", {\"val_haussdorf\": mean_val_haussdorf}, self.current_epoch)\n",
    "        self.logger.experiment.add_scalars(\"iou\", {\"val_iou\": mean_val_iou}, self.current_epoch)\n",
    "\n",
    "        # Save the best model\n",
    "        if mean_val_dice > self.best_val_dice:\n",
    "            self.best_val_dice = mean_val_dice\n",
    "            self.best_val_epoch = self.current_epoch\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch[\"img\"], batch[\"mask\"]\n",
    "        roi_size = VALIDATION_INFERENCE_ROI_SIZE\n",
    "        sw_batch_size = 4\n",
    "\n",
    "        outputs = sliding_window_inference(inputs, roi_size, sw_batch_size, self.forward, overlap=0.6)\n",
    "        loss = self.loss_function(outputs, labels)\n",
    "        outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
    "        labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
    "\n",
    "        # Dice metric\n",
    "        self.dice_metric(y_pred=outputs, y=labels)\n",
    "        volume_dice = self.dice_metric.aggregate().item()\n",
    "        self.dice_metric.reset()\n",
    "\n",
    "        # Surface dice metric\n",
    "        self.surface_dice_metric(y_pred=outputs, y=labels, spacing=SPACING)\n",
    "        volume_surface_dice = self.surface_dice_metric.aggregate().item()\n",
    "        self.surface_dice_metric.reset()\n",
    "\n",
    "        # Haussdorf metric\n",
    "        self.haussdorf_metric(y_pred=outputs, y=labels, spacing=SPACING)\n",
    "        volume_haussdorf = self.haussdorf_metric.aggregate().item()\n",
    "        self.haussdorf_metric.reset()\n",
    "\n",
    "        # IoU metric\n",
    "        self.iou_metric(y_pred=outputs, y=labels)\n",
    "        volume_iou = self.iou_metric.aggregate().item()\n",
    "        self.iou_metric.reset()\n",
    "\n",
    "        # Create a pandas dataframe with batch_idx, test_loss, and test_metric columns\n",
    "        df = pd.DataFrame({\n",
    "            \"volume\": [batch_idx],\n",
    "            \"test_loss\": [loss.item()],\n",
    "            \"test_dice\": [volume_dice],\n",
    "            \"test_surface_dice\": [volume_surface_dice],\n",
    "            \"test_haussdorf\": [volume_haussdorf],\n",
    "            \"test_iou\": [volume_iou]\n",
    "        })\n",
    "        self.test_dump_data_frame.append(df)\n",
    "        self.test_step_outputs.append({\"test_loss\": loss})\n",
    "\n",
    "        # Paint the figure\n",
    "        print(f\"len(outputs): {len(outputs)}\")\n",
    "        print(f\"len(labels): {len(labels)}\")\n",
    "        print(f\"shape of outputs: {outputs[0].shape}\")\n",
    "        print(f\"shape of labels: {labels[0].shape}\")\n",
    "    \n",
    "        output_writer = monai.data.NibabelWriter()\n",
    "        output_writer.set_data_array(outputs[0])\n",
    "        output_writer.write(f\"images/{self.test_ds.volumes[batch_idx]['img'].split('/')[-1].replace('.nii.gz','')}_output.nii.gz\")\n",
    "        \n",
    "        for j in range(outputs[0].shape[-1]):\n",
    "            burn_masks_in_ct(inputs[0, 0, :, :, j].cpu().numpy(), labels[0][0, :, :, j].cpu().numpy(), outputs[0][0, :, :, j].cpu().numpy(), path_to_save=f\"images/{self.test_ds.volumes[batch_idx]['img'].split('/')[-1].replace('.nii.gz','')}_slice{j}.png\")\n",
    "           \n",
    "        \n",
    "    def on_test_epoch_end(self):\n",
    "        df = pd.concat(self.test_dump_data_frame)\n",
    "        df.to_csv(EXPERIMENTS_PATH + self.experiment_name + \"/test_metrics.csv\", header=True, index=False)\n",
    "\n",
    "    def on_train_end(self) -> None:\n",
    "        self.metrics_df.to_csv(EXPERIMENTS_PATH + self.experiment_name + \"/train__val_metrics.csv\", header=True, index=False)\n",
    "    \n",
    "\n"
   ],
   "id": "8df1a6281c44c588",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T11:23:22.852586Z",
     "start_time": "2024-07-23T11:23:22.849139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# take path from lightning_logs\n",
    "path = '../Experiments/unetr_dice_2/checkpoints/epoch=178-step=7160.ckpt'\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "L.seed_everything(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "monai.utils.set_determinism(seed=SEED)\n",
    "\n",
    "# Experiment name\n",
    "experiment_name = f\"test\"\n",
    "print(f\"Experiment name: {experiment_name}\")\n"
   ],
   "id": "66adf0c6b5569c30",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name: test\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T11:23:56.968232Z",
     "start_time": "2024-07-23T11:23:23.195928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "net = Net(\n",
    "    learning_rate=1e-3,\n",
    "    model=experiments_items.nets.covid_unetr,\n",
    "    loss_function=DiceLoss(to_onehot_y=True, softmax=True, squared_pred=True),\n",
    "    volumes_path=\"../\" + COVID_PREPROCESSED_CASES_PATH,\n",
    "    masks_path=\"../\" + INFECTION_PREPROCESSED_MASKS_PATH,\n",
    "    experiment_name=experiment_name\n",
    ")\n",
    "\n",
    "\n",
    "tensorboard_logger = L.pytorch.loggers.TensorBoardLogger(save_dir=\"../\" + EXPERIMENTS_PATH, name=experiment_name, version=\"tensorboard\",)\n",
    "\n",
    "callbacks = [\n",
    "    L.pytorch.callbacks.ModelCheckpoint(\n",
    "        dirpath=\"../\" + EXPERIMENTS_PATH + experiment_name + \"/checkpoints\",\n",
    "        monitor=\"val_dice\",\n",
    "        save_top_k=1,\n",
    "        mode=\"max\",\n",
    "        save_last=True\n",
    "    ),\n",
    "    CustomTimingCallback()\n",
    "]\n",
    "\n",
    "trainer = L.pytorch.Trainer(\n",
    "    default_root_dir=\"../\" + EXPERIMENTS_PATH,\n",
    "    devices=[0],\n",
    "    accelerator=\"gpu\",\n",
    "    strategy=\"auto\",\n",
    "    max_epochs=200,\n",
    "    logger=tensorboard_logger,\n",
    "    callbacks=callbacks,\n",
    "    log_every_n_steps=14,\n",
    "    deterministic=True,\n",
    "    num_sanity_val_steps=0,\n",
    ")\n",
    "\n",
    "trainer.test(net)"
   ],
   "id": "8bf6eca3f231e964",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: <class 'monai.networks.nets.unetr.UNETR'>\n",
      "Using loss: <class 'monai.losses.dice.DiceLoss'>\n",
      "Using lr: 0.001\n",
      "Checking the test dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f978dcc311d44b2daa12daff4901c845"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(outputs): 1\n",
      "len(labels): 1\n",
      "shape of outputs: torch.Size([1, 577, 577, 51])\n",
      "shape of labels: torch.Size([1, 577, 577, 51])\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T21:11:29.363049Z",
     "start_time": "2024-07-22T21:11:28.362707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Example 3D tensor with shape (577, 577, 51) with 0s and 1s\n",
    "tensor = np.random.choice([0, 1], size=(577, 577, 51), p=[0.8, 0.2])\n",
    "\n",
    "# Ensure there's a mix of 0s and 1s\n",
    "print(\"Tensor shape:\", tensor.shape)\n",
    "\n",
    "# Dimensions of the tensor\n",
    "dims = tensor.shape\n",
    "\n",
    "# Create x, y, z coordinates for each voxel\n",
    "x = np.repeat(np.arange(dims[0]), dims[1] * dims[2])\n",
    "y = np.tile(np.repeat(np.arange(dims[1]), dims[2]), dims[0])\n",
    "z = np.tile(np.arange(dims[2]), dims[0] * dims[1])\n",
    "\n",
    "# Flatten the tensor to get the values for each voxel\n",
    "values = tensor.flatten()\n",
    "\n",
    "# Create a volume plot\n",
    "fig = go.Figure(data=go.Volume(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    z=z,\n",
    "    value=values,\n",
    "    opacity=0.2,  # Adjust opacity for better visualization\n",
    "    surface_count=17,  # Adjust for better visualization\n",
    "    colorscale='Viridis'  # Or any other color scale\n",
    "))\n",
    "\n",
    "# Set plot title and axis labels\n",
    "fig.update_layout(\n",
    "    title='3D Metatensor Volume Visualization',\n",
    "    scene=dict(\n",
    "        xaxis_title='X Axis',\n",
    "        yaxis_title='Y Axis',\n",
    "        zaxis_title='Z Axis'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ],
   "id": "fa6b428d628b4e7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape: (577, 577, 51)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[41], line 22\u001B[0m\n\u001B[1;32m     19\u001B[0m values \u001B[38;5;241m=\u001B[39m tensor\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Create a volume plot\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m fig \u001B[38;5;241m=\u001B[39m go\u001B[38;5;241m.\u001B[39mFigure(data\u001B[38;5;241m=\u001B[39mgo\u001B[38;5;241m.\u001B[39mVolume(\n\u001B[1;32m     23\u001B[0m     x\u001B[38;5;241m=\u001B[39mx,\n\u001B[1;32m     24\u001B[0m     y\u001B[38;5;241m=\u001B[39my,\n\u001B[1;32m     25\u001B[0m     z\u001B[38;5;241m=\u001B[39mz,\n\u001B[1;32m     26\u001B[0m     value\u001B[38;5;241m=\u001B[39mvalues,\n\u001B[1;32m     27\u001B[0m     opacity\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m,  \u001B[38;5;66;03m# Adjust opacity for better visualization\u001B[39;00m\n\u001B[1;32m     28\u001B[0m     surface_count\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m17\u001B[39m,  \u001B[38;5;66;03m# Adjust for better visualization\u001B[39;00m\n\u001B[1;32m     29\u001B[0m     colorscale\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mViridis\u001B[39m\u001B[38;5;124m'\u001B[39m  \u001B[38;5;66;03m# Or any other color scale\u001B[39;00m\n\u001B[1;32m     30\u001B[0m ))\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# Set plot title and axis labels\u001B[39;00m\n\u001B[1;32m     33\u001B[0m fig\u001B[38;5;241m.\u001B[39mupdate_layout(\n\u001B[1;32m     34\u001B[0m     title\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m3D Metatensor Volume Visualization\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     35\u001B[0m     scene\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     39\u001B[0m     )\n\u001B[1;32m     40\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/mi-env/lib/python3.11/site-packages/plotly/graph_objs/_figure.py:642\u001B[0m, in \u001B[0;36mFigure.__init__\u001B[0;34m(self, data, layout, frames, skip_invalid, **kwargs)\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28mself\u001B[39m, data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, layout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, frames\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, skip_invalid\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m      7\u001B[0m ):\n\u001B[1;32m      8\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124;03m    Create a new :class:Figure instance\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    640\u001B[0m \u001B[38;5;124;03m        is invalid AND skip_invalid is False\u001B[39;00m\n\u001B[1;32m    641\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 642\u001B[0m     \u001B[38;5;28msuper\u001B[39m(Figure, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(data, layout, frames, skip_invalid, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/mi-env/lib/python3.11/site-packages/plotly/basedatatypes.py:522\u001B[0m, in \u001B[0;36mBaseFigure.__init__\u001B[0;34m(self, data, layout_plotly, frames, skip_invalid, **kwargs)\u001B[0m\n\u001B[1;32m    517\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_objs \u001B[38;5;241m=\u001B[39m data\n\u001B[1;32m    519\u001B[0m \u001B[38;5;66;03m# ### Import clone of trace properties ###\u001B[39;00m\n\u001B[1;32m    520\u001B[0m \u001B[38;5;66;03m# The _data property is a list of dicts containing the properties\u001B[39;00m\n\u001B[1;32m    521\u001B[0m \u001B[38;5;66;03m# explicitly set by the user for each trace.\u001B[39;00m\n\u001B[0;32m--> 522\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data \u001B[38;5;241m=\u001B[39m [deepcopy(trace\u001B[38;5;241m.\u001B[39m_props) \u001B[38;5;28;01mfor\u001B[39;00m trace \u001B[38;5;129;01min\u001B[39;00m data]\n\u001B[1;32m    524\u001B[0m \u001B[38;5;66;03m# ### Create data defaults ###\u001B[39;00m\n\u001B[1;32m    525\u001B[0m \u001B[38;5;66;03m# _data_defaults is a tuple of dicts, one for each trace. When\u001B[39;00m\n\u001B[1;32m    526\u001B[0m \u001B[38;5;66;03m# running in a widget context, these defaults are populated with\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    530\u001B[0m \u001B[38;5;66;03m# Note: No property should exist in both the _data and\u001B[39;00m\n\u001B[1;32m    531\u001B[0m \u001B[38;5;66;03m# _data_defaults for the same trace.\u001B[39;00m\n\u001B[1;32m    532\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_defaults \u001B[38;5;241m=\u001B[39m [{} \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m data]\n",
      "File \u001B[0;32m~/miniconda3/envs/mi-env/lib/python3.11/site-packages/plotly/basedatatypes.py:522\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    517\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_objs \u001B[38;5;241m=\u001B[39m data\n\u001B[1;32m    519\u001B[0m \u001B[38;5;66;03m# ### Import clone of trace properties ###\u001B[39;00m\n\u001B[1;32m    520\u001B[0m \u001B[38;5;66;03m# The _data property is a list of dicts containing the properties\u001B[39;00m\n\u001B[1;32m    521\u001B[0m \u001B[38;5;66;03m# explicitly set by the user for each trace.\u001B[39;00m\n\u001B[0;32m--> 522\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data \u001B[38;5;241m=\u001B[39m [deepcopy(trace\u001B[38;5;241m.\u001B[39m_props) \u001B[38;5;28;01mfor\u001B[39;00m trace \u001B[38;5;129;01min\u001B[39;00m data]\n\u001B[1;32m    524\u001B[0m \u001B[38;5;66;03m# ### Create data defaults ###\u001B[39;00m\n\u001B[1;32m    525\u001B[0m \u001B[38;5;66;03m# _data_defaults is a tuple of dicts, one for each trace. When\u001B[39;00m\n\u001B[1;32m    526\u001B[0m \u001B[38;5;66;03m# running in a widget context, these defaults are populated with\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    530\u001B[0m \u001B[38;5;66;03m# Note: No property should exist in both the _data and\u001B[39;00m\n\u001B[1;32m    531\u001B[0m \u001B[38;5;66;03m# _data_defaults for the same trace.\u001B[39;00m\n\u001B[1;32m    532\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_defaults \u001B[38;5;241m=\u001B[39m [{} \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m data]\n",
      "File \u001B[0;32m~/miniconda3/envs/mi-env/lib/python3.11/copy.py:146\u001B[0m, in \u001B[0;36mdeepcopy\u001B[0;34m(x, memo, _nil)\u001B[0m\n\u001B[1;32m    144\u001B[0m copier \u001B[38;5;241m=\u001B[39m _deepcopy_dispatch\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28mcls\u001B[39m)\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m copier \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 146\u001B[0m     y \u001B[38;5;241m=\u001B[39m copier(x, memo)\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    148\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28missubclass\u001B[39m(\u001B[38;5;28mcls\u001B[39m, \u001B[38;5;28mtype\u001B[39m):\n",
      "File \u001B[0;32m~/miniconda3/envs/mi-env/lib/python3.11/copy.py:231\u001B[0m, in \u001B[0;36m_deepcopy_dict\u001B[0;34m(x, memo, deepcopy)\u001B[0m\n\u001B[1;32m    229\u001B[0m memo[\u001B[38;5;28mid\u001B[39m(x)] \u001B[38;5;241m=\u001B[39m y\n\u001B[1;32m    230\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m x\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m--> 231\u001B[0m     y[deepcopy(key, memo)] \u001B[38;5;241m=\u001B[39m deepcopy(value, memo)\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m y\n",
      "File \u001B[0;32m~/miniconda3/envs/mi-env/lib/python3.11/copy.py:153\u001B[0m, in \u001B[0;36mdeepcopy\u001B[0;34m(x, memo, _nil)\u001B[0m\n\u001B[1;32m    151\u001B[0m copier \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(x, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__deepcopy__\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m copier \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 153\u001B[0m     y \u001B[38;5;241m=\u001B[39m copier(memo)\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    155\u001B[0m     reductor \u001B[38;5;241m=\u001B[39m dispatch_table\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28mcls\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T22:16:16.282322Z",
     "start_time": "2024-07-21T22:16:16.280289Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2f0bef6739248422",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "336c7cc420988f4c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
