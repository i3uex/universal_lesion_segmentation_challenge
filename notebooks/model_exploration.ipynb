{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-08T10:45:35.035826Z",
     "start_time": "2024-07-08T10:45:35.032734Z"
    }
   },
   "source": [
    "import logging\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from lightning.pytorch.utilities.types import TRAIN_DATALOADERS\n",
    "from monai.inferers import sliding_window_inference\n",
    "\n",
    "import experiments_items.nets\n",
    "from monai.metrics.meandice import compute_dice\n",
    "from config.config import Config\n",
    "from preprocessing.covid_dataset import CovidDataset\n",
    "import monai.data\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import DiceLoss\n",
    "from preprocessing.transforms import get_hrct_transforms, get_cbct_transforms, \\\n",
    "    get_val_hrct_transforms, get_val_cbct_transforms\n",
    "from utils.custom_callbacks import CustomTimingCallback\n",
    "from utils.helpers import load_images_from_path, check_dataset\n",
    "from config.constants import (COVID_CASES_PATH, INFECTION_MASKS_PATH, SEED, VALIDATION_INFERENCE_ROI_SIZE, SPATIAL_SIZE,\n",
    "                              LUNG_MASKS_PATH)\n",
    "import torch\n",
    "import numpy as np\n",
    "from monai.metrics import DiceMetric\n",
    "import lightning as L\n"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T11:12:52.777180Z",
     "start_time": "2024-07-08T11:12:52.760371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Net(L.pytorch.LightningModule):\n",
    "    def __init__(self, learning_rate: float, model: torch.nn.Module, volumes_path: str,\n",
    "                 masks_path: str):\n",
    "        super(Net, self).__init__()\n",
    "        print(f\"Using lr: {learning_rate}\")\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.save_hyperparameters(ignore=\"model\")\n",
    "\n",
    "        self.post_pred = monai.transforms.Compose(\n",
    "            [monai.transforms.EnsureType(data_type='tensor'), monai.transforms.Activations(sigmoid=True), monai.transforms.AsDiscrete(threshold=0.5)])\n",
    "        self.post_label = monai.transforms.Compose([monai.transforms.AsDiscrete(threshold=0.5)])\n",
    "\n",
    "        self.best_val_dice = 0\n",
    "        self.best_val_epoch = 0\n",
    "\n",
    "        self.validation_step_outputs = []\n",
    "        self.train_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "        self.test_paths = None\n",
    "        self.val_paths = None\n",
    "        self.train_paths = None\n",
    "\n",
    "        self.training_ds = None\n",
    "        self.validation_ds = None\n",
    "        self.test_ds = None\n",
    "\n",
    "        self.volumes_path = volumes_path\n",
    "        self.masks_path = masks_path\n",
    "\n",
    "        self.model = model\n",
    "        print(f\"Using model: {type(self.model)}\")\n",
    "        self.dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "        self.train_dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "        self.loss_function = monai.losses.DiceLoss(sigmoid=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # Load images and masks\n",
    "        logging.info(f\"Loading images from {self.volumes_path} and masks from {self.masks_path}\")\n",
    "        images = load_images_from_path(self.volumes_path)\n",
    "        labels = load_images_from_path(self.masks_path)\n",
    "\n",
    "        # Convert images and masks to a list of dictionaries with keys \"img\" and \"mask\"\n",
    "        data_dicts = np.array([{\"img\": img, \"mask\": mask} for img, mask in zip(images, labels)])\n",
    "        logging.debug(data_dicts)\n",
    "\n",
    "        shuffler = np.random.RandomState(SEED)\n",
    "        shuffler.shuffle(data_dicts)\n",
    "        data_dicts = list(data_dicts)\n",
    "\n",
    "        # Split the data into training (70%), validation (20%), and test sets (10%)\n",
    "        test_split = int(len(data_dicts) * 0.1)\n",
    "        val_split = int(len(data_dicts) * 0.2)\n",
    "\n",
    "        self.train_paths = data_dicts[test_split + val_split:]\n",
    "        self.val_paths = data_dicts[test_split:test_split + val_split]\n",
    "        self.test_paths = data_dicts[:test_split]\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            # Define the CovidDataset instances for training, validation, and test\n",
    "            self.training_ds = CovidDataset(volumes=self.train_paths, hrct_transform=get_hrct_transforms(),\n",
    "                                            cbct_transform=get_cbct_transforms())\n",
    "            self.validation_ds = CovidDataset(volumes=self.val_paths, hrct_transform=get_val_hrct_transforms(),\n",
    "                                              cbct_transform=get_val_cbct_transforms())\n",
    "            # Check the dataset\n",
    "            print(\"Checking the dataset\")\n",
    "            check_dataset(self.validation_ds)\n",
    "\n",
    "        if stage == \"validate\" or stage is None:\n",
    "            self.validation_ds = CovidDataset(volumes=self.val_paths, hrct_transform=get_val_hrct_transforms(),\n",
    "                                              cbct_transform=get_val_cbct_transforms())\n",
    "            # Check the dataset\n",
    "            print(\"Checking the dataset\")\n",
    "            check_dataset(self.validation_ds)\n",
    "\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_ds = CovidDataset(volumes=self.test_paths, hrct_transform=get_val_hrct_transforms(),\n",
    "                                        cbct_transform=get_val_cbct_transforms())\n",
    "            # Check the dataset\n",
    "            print(\"Checking the dataset\")\n",
    "            check_dataset(self.test_ds)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataloader = DataLoader(self.training_ds, batch_size=1, shuffle=True, num_workers=os.cpu_count())\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataloader = DataLoader(self.validation_ds, batch_size=1, num_workers=os.cpu_count())\n",
    "        return val_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_dataloader = DataLoader(self.test_ds, batch_size=1, num_workers=os.cpu_count())\n",
    "        return test_dataloader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch[\"img\"], batch[\"mask\"] #.cuda(), batch[\"mask\"].cuda()\n",
    "        raw_outputs = self.forward(inputs)\n",
    "        loss = self.loss_function(raw_outputs, labels)\n",
    "        self.log(\"train_step_loss\", loss, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        outputs = [self.post_pred(i) for i in decollate_batch(raw_outputs)]\n",
    "        labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
    "        self.train_dice_metric(y_pred=outputs, y=labels)\n",
    "\n",
    "        self.log(\"train_step_dice\", self.train_dice_metric.aggregate().item(), on_step=True, on_epoch=False, prog_bar=True)\n",
    "\n",
    "        train_loss_dictionary = {\"loss\": loss}\n",
    "        self.train_step_outputs.append(train_loss_dictionary)\n",
    "        return train_loss_dictionary\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        avg_loss = torch.stack([i[\"loss\"] for i in self.train_step_outputs]).mean()\n",
    "        mean_train_dice = self.train_dice_metric.aggregate().item()\n",
    "        self.train_dice_metric.reset()\n",
    "\n",
    "        self.log_dict({\"train_dice\": mean_train_dice, \"train_loss\": avg_loss}, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            \"train_dice\": mean_train_dice,\n",
    "            \"train_loss\": avg_loss,\n",
    "        }\n",
    "\n",
    "        self.logger.experiment.add_scalars(\"losses\", {\"train\": avg_loss}, self.current_epoch)\n",
    "        self.logger.experiment.add_scalars(\"dice\", {\"train\": mean_train_dice}, self.current_epoch)\n",
    "        self.logger.log_metrics(tensorboard_logs, step=self.current_epoch)\n",
    "\n",
    "        self.train_step_outputs.clear()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch[\"img\"], batch[\"mask\"]\n",
    "        roi_size = VALIDATION_INFERENCE_ROI_SIZE\n",
    "        sw_batch_size = 4\n",
    "\n",
    "        outputs = sliding_window_inference(inputs, roi_size, sw_batch_size, self.forward, overlap=0.3)\n",
    "        loss = self.loss_function(outputs, labels)\n",
    "        outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
    "        labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
    "        self.dice_metric(y_pred=outputs, y=labels)\n",
    "\n",
    "        validation_loss_dictionary = {\"val_loss\": loss}\n",
    "        self.validation_step_outputs.append(validation_loss_dictionary)\n",
    "        return validation_loss_dictionary\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in self.validation_step_outputs]).mean()\n",
    "        mean_val_dice = self.dice_metric.aggregate().item()\n",
    "        self.dice_metric.reset()\n",
    "\n",
    "        self.log_dict({\"val_dice\": mean_val_dice, \"val_loss\": avg_loss}, prog_bar=True, on_epoch=True, on_step=False) #sync_dist=True)\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            \"val_dice\": mean_val_dice,\n",
    "            \"val_loss\": avg_loss,\n",
    "        }\n",
    "\n",
    "        self.logger.experiment.add_scalars(\"losses\", {\"val_loss\": avg_loss}, self.current_epoch)\n",
    "        self.logger.experiment.add_scalars(\"dice\", {\"val_dice\": mean_val_dice}, self.current_epoch)\n",
    "        self.logger.log_metrics(tensorboard_logs, step=self.current_epoch)\n",
    "\n",
    "        if mean_val_dice > self.best_val_dice:\n",
    "            self.best_val_dice = mean_val_dice\n",
    "            self.best_val_epoch = self.current_epoch\n",
    "\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch[\"img\"], batch[\"mask\"]\n",
    "        roi_size = VALIDATION_INFERENCE_ROI_SIZE\n",
    "        sw_batch_size = 4\n",
    "\n",
    "        outputs = sliding_window_inference(inputs, roi_size, sw_batch_size, self.forward, overlap=0.6)\n",
    "        loss = self.loss_function(outputs, labels)\n",
    "        outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
    "        labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
    "        self.dice_metric(y_pred=outputs, y=labels)\n",
    "\n",
    "        self.test_step_outputs.append({\"test_loss\": loss})\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        test_loss = torch.stack([x[\"test_loss\"] for x in self.test_step_outputs]).mean()\n",
    "        test_dice = self.dice_metric.aggregate().item()\n",
    "        self.dice_metric.reset()\n",
    "\n",
    "        self.log_dict({\"test_dice\": test_dice, \"test_loss\": test_loss}, prog_bar=True)\n",
    "\n",
    "\n"
   ],
   "id": "8df1a6281c44c588",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T11:12:53.103214Z",
     "start_time": "2024-07-08T11:12:53.101638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# take path from lightning_logs\n",
    "path = '../lightning_logs/lightning_logs/version_138/checkpoints/epoch=1763-step=24696.ckpt'"
   ],
   "id": "66adf0c6b5569c30",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T11:12:53.739951Z",
     "start_time": "2024-07-08T11:12:53.551992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# lest test the checkpoint numebr 138\n",
    "aux = UNet(\n",
    "            spatial_dims=3,\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            channels=(16, 32, 64, 128),\n",
    "            strides=(2, 2, 2),\n",
    "            num_res_units=2,\n",
    "            dropout=0.3\n",
    "        )\n",
    "\n",
    "model = Net.load_from_checkpoint(path, model=aux, learning_rate=1e-3, volumes_path=\"../\" + COVID_CASES_PATH, masks_path=\"../\" + INFECTION_MASKS_PATH)\n"
   ],
   "id": "c7e05e9ffb6f6cf2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lr: 0.001\n",
      "Using model: <class 'monai.networks.nets.unet.UNet'>\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T11:13:01.170745Z",
     "start_time": "2024-07-08T11:12:54.047924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "model.prepare_data()\n",
    "model.setup(\"test\")\n",
    "print(model.test_ds)"
   ],
   "id": "2d45c1313cfe5e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the dataset\n",
      "<preprocessing.covid_dataset.CovidDataset object at 0x7c2748b0bc90>\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T11:13:32.789598Z",
     "start_time": "2024-07-08T11:13:01.171738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensorboard_logger = (L.pytorch.loggers.TensorBoardLogger(save_dir=\"lightning_logs\", name=\"lightning_logs\"))\n",
    "callbacks = [L.pytorch.callbacks.ModelCheckpoint(monitor=\"val_dice\", save_top_k=1, mode=\"max\", save_last=True), CustomTimingCallback()]\n",
    "\n",
    "trainer = L.pytorch.Trainer(\n",
    "    devices=[0],\n",
    "    accelerator=\"gpu\",\n",
    "    strategy=\"auto\",\n",
    "    logger=tensorboard_logger,\n",
    "    callbacks=callbacks,\n",
    "    log_every_n_steps=14,\n",
    "    deterministic=True,\n",
    "    num_sanity_val_steps=0,\n",
    ")\n",
    "\n",
    "trainer.test(model=model, dataloaders=model.test_dataloader(), ckpt_path=path)"
   ],
   "id": "8bf6eca3f231e964",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at ../lightning_logs/lightning_logs/version_138/checkpoints/epoch=1763-step=24696.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loaded model weights from the checkpoint at ../lightning_logs/lightning_logs/version_138/checkpoints/epoch=1763-step=24696.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30685ae70f684291b83894d944ca67cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001B[36m \u001B[0m\u001B[36m        test_dice        \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.6277163624763489    \u001B[0m\u001B[35m \u001B[0m│\n",
       "│\u001B[36m \u001B[0m\u001B[36m        test_loss        \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.37258782982826233   \u001B[0m\u001B[35m \u001B[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_dice         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6277163624763489     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.37258782982826233    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_dice': 0.6277163624763489, 'test_loss': 0.37258782982826233}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T11:11:13.986758Z",
     "start_time": "2024-07-08T11:11:13.962161Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "df0926f341d84198",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Trainer' object has no attribute 'best_model_path'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[58], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m trainer\u001B[38;5;241m.\u001B[39mbest_model_path\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Trainer' object has no attribute 'best_model_path'"
     ]
    }
   ],
   "execution_count": 58
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
